{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.准备环境与路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CreateSparkContext():\n",
    "#     '''\n",
    "#     spark配置：\n",
    "#     1.显示在spark或 hadoop-yarn UI界面的App名称\n",
    "#     2.设置不显示spark执行进度以免界面太乱\n",
    "#     '''\n",
    "#     sparkConf = SparkConf().setAppName('Finacial Distress Prediction')  \\\n",
    "#             .set('spark.ui.showConsoleProgress','false') \n",
    "#     sc = SparkContext(conf=sparkConf)\n",
    "#     print('master='+sc.master)\n",
    "#     SetLogger(sc) #设置不要显示太多信息\n",
    "#     SetPath(sc) # 设置文件读取路径\n",
    "#     return sc\n",
    "\n",
    "# def SetLogger(sc):\n",
    "#     logger = sc._jvm.org.apache.log4j\n",
    "#     logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "#     logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "#     logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)\n",
    "    \n",
    "def SetPath(sc):\n",
    "    if sc.master[0:5]==\"local\":\n",
    "        Path = \"file:/home/hadoop/eclipse-workspace/FinacialDistress/\"\n",
    "    else:\n",
    "        Path = \"hdfs://ubuntu:9000/sparkproject/FinacialDistress/\"\n",
    "    return Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()\n",
    "# spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "Path = SetPath(sc)\n",
    "# Path = \"file:/home/hadoop/eclipse-workspace/FinacialDistress/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.读取数据并整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .load(Path+\"data/FinacialDistress.csv\")\n",
    "\n",
    "# raw_df = spark.read.csv(Path+\"data/FinacialDistress.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3672"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Financial Distress: string (nullable = true)\n",
      " |-- x1: string (nullable = true)\n",
      " |-- x2: string (nullable = true)\n",
      " |-- x3: string (nullable = true)\n",
      " |-- x4: string (nullable = true)\n",
      " |-- x5: string (nullable = true)\n",
      " |-- x6: string (nullable = true)\n",
      " |-- x7: string (nullable = true)\n",
      " |-- x8: string (nullable = true)\n",
      " |-- x9: string (nullable = true)\n",
      " |-- x10: string (nullable = true)\n",
      " |-- x11: string (nullable = true)\n",
      " |-- x12: string (nullable = true)\n",
      " |-- x13: string (nullable = true)\n",
      " |-- x14: string (nullable = true)\n",
      " |-- x15: string (nullable = true)\n",
      " |-- x16: string (nullable = true)\n",
      " |-- x17: string (nullable = true)\n",
      " |-- x18: string (nullable = true)\n",
      " |-- x19: string (nullable = true)\n",
      " |-- x20: string (nullable = true)\n",
      " |-- x21: string (nullable = true)\n",
      " |-- x22: string (nullable = true)\n",
      " |-- x23: string (nullable = true)\n",
      " |-- x24: string (nullable = true)\n",
      " |-- x25: string (nullable = true)\n",
      " |-- x26: string (nullable = true)\n",
      " |-- x27: string (nullable = true)\n",
      " |-- x28: string (nullable = true)\n",
      " |-- x29: string (nullable = true)\n",
      " |-- x30: string (nullable = true)\n",
      " |-- x31: string (nullable = true)\n",
      " |-- x32: string (nullable = true)\n",
      " |-- x33: string (nullable = true)\n",
      " |-- x34: string (nullable = true)\n",
      " |-- x35: string (nullable = true)\n",
      " |-- x36: string (nullable = true)\n",
      " |-- x37: string (nullable = true)\n",
      " |-- x38: string (nullable = true)\n",
      " |-- x39: string (nullable = true)\n",
      " |-- x40: string (nullable = true)\n",
      " |-- x41: string (nullable = true)\n",
      " |-- x42: string (nullable = true)\n",
      " |-- x43: string (nullable = true)\n",
      " |-- x44: string (nullable = true)\n",
      " |-- x45: string (nullable = true)\n",
      " |-- x46: string (nullable = true)\n",
      " |-- x47: string (nullable = true)\n",
      " |-- x48: string (nullable = true)\n",
      " |-- x49: string (nullable = true)\n",
      " |-- x50: string (nullable = true)\n",
      " |-- x51: string (nullable = true)\n",
      " |-- x52: string (nullable = true)\n",
      " |-- x53: string (nullable = true)\n",
      " |-- x54: string (nullable = true)\n",
      " |-- x55: string (nullable = true)\n",
      " |-- x56: string (nullable = true)\n",
      " |-- x57: string (nullable = true)\n",
      " |-- x58: string (nullable = true)\n",
      " |-- x59: string (nullable = true)\n",
      " |-- x60: string (nullable = true)\n",
      " |-- x61: string (nullable = true)\n",
      " |-- x62: string (nullable = true)\n",
      " |-- x63: string (nullable = true)\n",
      " |-- x64: string (nullable = true)\n",
      " |-- x65: string (nullable = true)\n",
      " |-- x66: string (nullable = true)\n",
      " |-- x67: string (nullable = true)\n",
      " |-- x68: string (nullable = true)\n",
      " |-- x69: string (nullable = true)\n",
      " |-- x70: string (nullable = true)\n",
      " |-- x71: string (nullable = true)\n",
      " |-- x72: string (nullable = true)\n",
      " |-- x73: string (nullable = true)\n",
      " |-- x74: string (nullable = true)\n",
      " |-- x75: string (nullable = true)\n",
      " |-- x76: string (nullable = true)\n",
      " |-- x77: string (nullable = true)\n",
      " |-- x78: string (nullable = true)\n",
      " |-- x79: string (nullable = true)\n",
      " |-- x80: string (nullable = true)\n",
      " |-- x81: string (nullable = true)\n",
      " |-- x82: string (nullable = true)\n",
      " |-- x83: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+-------+---------+\n",
      "|Company|Time|Financial Distress|     x1|       x2|\n",
      "+-------+----+------------------+-------+---------+\n",
      "|      1|   1|          0.010636|  1.281| 0.022934|\n",
      "|      1|   2|          -0.45597|   1.27|0.0064542|\n",
      "|      1|   3|          -0.32539| 1.0529|-0.059379|\n",
      "|      1|   4|          -0.56657| 1.1131|-0.015229|\n",
      "|      2|   1|            1.3573| 1.0623|  0.10702|\n",
      "|      2|   2|         0.0071875| 1.0558| 0.081916|\n",
      "|      2|   3|            1.2002|0.97059| 0.076064|\n",
      "|      2|   4|            2.2348|  1.059|   0.1302|\n",
      "|      2|   5|            1.3405| 1.1245|  0.14784|\n",
      "|      2|   6|            2.0474| 1.5998|  0.26246|\n",
      "|      2|   7|            2.3459| 1.5756|   0.2621|\n",
      "|      2|   8|            2.2499| 1.5443|  0.24091|\n",
      "|      2|   9|            2.2826| 1.7217|  0.21525|\n",
      "|      2|  10|            1.7982| 1.7232|  0.20863|\n",
      "|      2|  11|            2.8277| 1.6307|  0.18623|\n",
      "|      2|  12|            2.9413| 2.1723|  0.31945|\n",
      "|      2|  13|            2.6598| 2.0924|  0.30348|\n",
      "|      2|  14|             2.232| 2.2263|  0.32588|\n",
      "|      3|   1|           -1.6599| 0.8744|-0.034676|\n",
      "|      4|   1|           0.02275|0.83537|  0.06431|\n",
      "+-------+----+------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.select('Company','Time','Financial Distress','x1','x2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "import pyspark.sql.types\n",
    "def replace_question(col):\n",
    "    return ('0' if col==\"?\" else col)\n",
    "replace_question = udf(replace_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "import pyspark.sql.types\n",
    "def replace_question(col): # df.na.fill() 或者df.fillna()\n",
    "    return ('0' if col==\"?\" else col)\n",
    "replace_question = udf(replace_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = raw_df.select(['Company']+[replace_question(col(\"Time\")).cast('int').alias('Time')] +\n",
    "                       [replace_question(col(c)).cast('double').alias(c) for c in raw_df.columns[2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = rawDF.withColumn('Financial Distress01',(rawDF['Financial Distress']>-0.5).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = df.randomSplit([0.7,0.3])\n",
    "# train_df.cache()\n",
    "# test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6996187363834423"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()/df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.处理字符串分类特征\n",
    "* 利用StringIndexer将文字分类特征转换为数值分类特征\n",
    "* 利用OneHotEncoder将数值分类特征转换为Vector如(0,0,0,1,0,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "# # -----StringIndexer----\n",
    "# category = StringIndexer(inputCol='inputStr',outputCOl='outputNum') #StringIndexer是一个Estimator\n",
    "# categoryTransformer = category.fit(df) #调用fit方法后生成一个Transformer\n",
    "# categoryTransformer.labels #即为分类字符串\n",
    "# #将Transformer应用到train_df和test_df\n",
    "# trainDF = categoryTransformer.transform(train_df)\n",
    "# testDF = categoryTransformer.transform(test_df)\n",
    "\n",
    "# # -------OneHotEncoder------\n",
    "# encoder = OneHotEncoder(inputCol='outputNum',outputCol='outputVec',dropLast=False)\n",
    "# train_DF = encoder.transform(trainDF)\n",
    "# testDF = encoder.transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.使用 VectorAssembler 将所有特征集成为一个特征Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assemblers = df.columns[3:-1]\n",
    "vecAssembler = VectorAssembler(inputCols = assemblers,outputCol= 'Features')\n",
    "trainDF = vecAssembler.transform(train_df)\n",
    "# testDF = vecAssembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Features|\n",
      "+--------------------+\n",
      "|[1.281,0.022934,0...|\n",
      "|[1.1131,-0.015229...|\n",
      "|[1.4809,0.11739,0...|\n",
      "|[1.3854,0.1013,0....|\n",
      "|[1.267,0.053775,0...|\n",
      "|[1.2675,0.059595,...|\n",
      "|[1.3881,0.10079,0...|\n",
      "|[1.4323,0.083595,...|\n",
      "|[1.3399,0.066151,...|\n",
      "|[1.3167,0.061076,...|\n",
      "|[1.3457,0.049124,...|\n",
      "|[1.3107,0.028949,...|\n",
      "|[1.3833,0.087312,...|\n",
      "|[1.3833,0.013043,...|\n",
      "|[1.0246,0.13107,0...|\n",
      "|[1.1007,0.059918,...|\n",
      "|[1.3134,0.065683,...|\n",
      "|[0.98085,0.014711...|\n",
      "|[0.88577,0.051745...|\n",
      "|[0.94864,0.040847...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Features=DenseVector([1.281, 0.0229, 0.8745, 1.2164, 0.0609, 0.1883, 0.5251, 0.0189, 0.1828, 0.0064, 0.8582, 2.0058, 0.1255, 6.9706, 4.6512, 0.0501, 2.1984, 0.0183, 0.025, 0.0273, 1.4173, 9.5554, 0.1487, 0.67, 214.76, 12.641, 6.4607, 0.0438, 0.2046, 0.3518, 8.3161, 0.2892, 0.7661, 2.5825, 77.4, 0.0267, 1.6307, 0.015, 0.0055, 0.1273, 9.6951, -0.7362, 0.9856, 0.1802, 1.5006, 0.0262, 7.0513, 1174.9, 5.3399, 0.8513, 12.837, 0.0617, 0.1809, 209.87, -0.5826, 0.471, 0.1099, 0.0, 0.0, 0.2201, 7.1241, 15.381, 3.2702, 17.872, 34.692, 30.087, 12.8, 7991.4, 364.95, 15.8, 61.476, 4.0, 36.0, 85.437, 27.07, 26.102, 16.0, 16.0, 0.2, 22.0, 0.0604, 30.0, 49.0]))]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select('Features').show()\n",
    "trainDF.select('Features').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.使用DecisionTreeClassifier二元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtClassifier = DecisionTreeClassifier(labelCol='Financial Distress01',featuresCol='Features',impurity='gini',maxDepth=10,maxBins=14)\n",
    "dtModel = dtClassifier.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPredict = dtModel.transform(testDF) #预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, Time: int, Financial Distress: double, x1: double, x2: double, x3: double, x4: double, x5: double, x6: double, x7: double, x8: double, x9: double, x10: double, x11: double, x12: double, x13: double, x14: double, x15: double, x16: double, x17: double, x18: double, x19: double, x20: double, x21: double, x22: double, x23: double, x24: double, x25: double, x26: double, x27: double, x28: double, x29: double, x30: double, x31: double, x32: double, x33: double, x34: double, x35: double, x36: double, x37: double, x38: double, x39: double, x40: double, x41: double, x42: double, x43: double, x44: double, x45: double, x46: double, x47: double, x48: double, x49: double, x50: double, x51: double, x52: double, x53: double, x54: double, x55: double, x56: double, x57: double, x58: double, x59: double, x60: double, x61: double, x62: double, x63: double, x64: double, x65: double, x66: double, x67: double, x68: double, x69: double, x70: double, x71: double, x72: double, x73: double, x74: double, x75: double, x76: double, x77: double, x78: double, x79: double, x80: double, x81: double, x82: double, x83: double, Financial Distress01: int, Features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtPredict.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.建立机器学习流程pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_419c8201a2864fdbda45,\n",
       " DecisionTreeClassifier_4c8989b34e2c2d43789c]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler,dtClassifier])\n",
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes\n",
      "  If (feature 45 <= 0.0022849999999999997)\n",
      "   If (feature 2 <= 0.734785)\n",
      "    If (feature 22 <= 0.17000500000000002)\n",
      "     If (feature 61 <= 12.175)\n",
      "      If (feature 19 <= 0.25765499999999997)\n",
      "       If (feature 60 <= 6.92695)\n",
      "        If (feature 68 <= -46.2455)\n",
      "         If (feature 16 <= 1.3288)\n",
      "          Predict: 0.0\n",
      "         Else (feature 16 > 1.3288)\n",
      "          Predict: 1.0\n",
      "        Else\n"
     ]
    }
   ],
   "source": [
    "print(pipelineModel.stages[1].toDebugString[:500]) #查看决策树规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePredict = pipelineModel.transform(test_df) #预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|Company|Time|            Features|Financial Distress01|rawPrediction|         probability|prediction|\n",
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|      1|   2|[1.27,0.0064542,0...|                   1|    [0.0,4.0]|           [0.0,1.0]|       1.0|\n",
      "|      1|   3|[1.0529,-0.059379...|                   1|   [15.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|     10|  12|[1.5795,0.17996,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|     10|  13|[1.5012,0.078785,...|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|   2|[0.79696,0.090335...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|   6|[0.9303,0.084578,...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|  11|[1.1898,0.10814,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|  12|[1.5872,0.0024181...|                   1|    [3.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    100|  13|[1.3346,-0.1506,0...|                   1|    [0.0,6.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   2|[1.1733,0.11988,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   3|[1.1295,0.053808,...|                   1|  [0.0,208.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   4|[2.2288,0.27663,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   6|[1.3375,0.16029,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   9|[1.0738,0.12851,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|   2|[1.0663,0.0688,0....|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|   8|[1.6494,0.11315,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|  10|[0.9068,0.06267,0...|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|  12|[1.6099,0.1366,0....|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    104|   3|[1.0203,0.030599,...|                   1|    [0.0,9.0]|           [0.0,1.0]|       1.0|\n",
      "|    104|   5|[1.136,0.07224,0....|                   1|  [3.0,181.0]|[0.01630434782608...|       1.0|\n",
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelinePredict.select('Company','Time','Features','Financial Distress01','rawPrediction','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.评估模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',\n",
    "                                          labelCol='Financial Distress01',\n",
    "                                          metricName='areaUnderROC')\n",
    "auc = evaluator.evaluate(pipelinePredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042644896303433"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.寻找最佳模型\n",
    "\n",
    "## 8.1使用TrainValidationSplit寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder().addGrid(dtClassifier.impurity,[\"gini\",\"entropy\"]) \\\n",
    "                .addGrid(dtClassifier.maxDepth,[5,10,15,20]) \\\n",
    "                .addGrid(dtClassifier.maxBins,[10,15,20,25]).build()\n",
    "dtTVS = TrainValidationSplit(estimator = dtClassifier,evaluator=evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "dtTVS_pipeline = Pipeline(stages=[vecAssembler,dtTVS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtTVS_models = dtTVS_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 53 nodes"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = dtTVS_models.stages[1].bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 53 nodes\n",
      "  If (feature 8 <= -0.114315)\n",
      "   If (feature 57 <= -9.549999999999999E-5)\n",
      "    If (feature 10 <= 0.50169)\n",
      "     If (feature 64 <= 34.1225)\n",
      "      Predict: 1.0\n",
      "     Else (feature 64 > 34.1225)\n",
      "      If (feature 0 <= 0.7623949999999999)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 0.7623949999999999)\n",
      "       Predict: 1.0\n",
      "    Else (feature 10 > 0.50169)\n",
      "     If (feature 81 <= 12.5)\n",
      "      Predict: 0\n"
     ]
    }
   ],
   "source": [
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9033390801683483"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtTVS_predict = dtTVS_models.transform(test_df)\n",
    "auc = evaluator.evaluate(dtTVS_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2使用crossValidation寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "dtCV = CrossValidator(estimator=dtClassifier,evaluator=evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "dtCV_pipeline = Pipeline(stages=[vecAssembler,dtCV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtCV_models = dtCV_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 55 nodes"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel_ = dtCV_models.stages[1].bestModel\n",
    "bestModel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9061279853962781"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtCV_predict = dtCV_models.transform(test_df)\n",
    "auc = evaluator.evaluate(dtCV_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9251559251559249"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier(labelCol = 'Financial Distress01',featuresCol='Features',numTrees = 10)\n",
    "rf_pipeline = Pipeline(stages=[vecAssembler,rfClassifier])\n",
    "rf_models = rf_pipeline.fit(train_df)\n",
    "rfPredict = rf_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfPredict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 使用TrainValidationSplit寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9351706302925814"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(rfClassifier.impurity,[\"gini\",\"entropy\"]) \\\n",
    "                .addGrid(rfClassifier.maxDepth,[5,10,15,20]) \\\n",
    "                .addGrid(rfClassifier.maxBins,[10,15,20,25]) \\\n",
    "                .addGrid(rfClassifier.numTrees,[10,20,30]).build()\n",
    "rfTVS = TrainValidationSplit(estimator = rfClassifier,evaluator=evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "rfTVS_pipeline  = Pipeline(stages=[vecAssembler,rfTVS])\n",
    "rfTVS_models = rfTVS_pipeline.fit(train_df)\n",
    "rfTVS_predict = rfTVS_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfTVS_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2使用crossValidation寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfCV = CrossValidator(estimator=rfClassifier,evaluator=evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "rfCV_pipeline = Pipeline(stages=[vecAssembler,rfCV])\n",
    "rfCV_models = rfCV_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9351706302925814"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfCV_predict = rfCV_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfCV_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 决策树回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dtReg = DecisionTreeRegressor(labelCol='Financial Distress',featuresCol='Features')\n",
    "dtReg_pipeline = Pipeline(stages=[vecAssembler,dtReg])\n",
    "dtReg_model = dtReg_pipeline.fit(train_df)\n",
    "dtReg_predict = dtReg_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+------------------+--------------------+\n",
      "|Company|Time|            Features|Financial Distress|          prediction|\n",
      "+-------+----+--------------------+------------------+--------------------+\n",
      "|      1|   2|[1.27,0.0064542,0...|          -0.45597|-0.42818420844155847|\n",
      "|      1|   3|[1.0529,-0.059379...|          -0.32539|-0.42818420844155847|\n",
      "|     10|  12|[1.5795,0.17996,0...|            1.1553|  0.8798071445645164|\n",
      "|     10|  13|[1.5012,0.078785,...|           0.31012|  0.8798071445645164|\n",
      "|    100|   2|[0.79696,0.090335...|           0.13046| 0.22446443815899586|\n",
      "|    100|   6|[0.9303,0.084578,...|           0.47346| 0.22446443815899586|\n",
      "|    100|  11|[1.1898,0.10814,0...|           0.04456|  0.4636442058004643|\n",
      "|    100|  12|[1.5872,0.0024181...|           -0.1284|0.012763109456521747|\n",
      "|    100|  13|[1.3346,-0.1506,0...|          -0.48243|-0.42818420844155847|\n",
      "|    101|   2|[1.1733,0.11988,0...|            1.2129|   1.411190434322034|\n",
      "|    101|   3|[1.1295,0.053808,...|            1.0134|  0.8798071445645164|\n",
      "|    101|   4|[2.2288,0.27663,0...|            1.3322|  0.8798071445645164|\n",
      "|    101|   6|[1.3375,0.16029,0...|            1.1882|   1.411190434322034|\n",
      "|    101|   9|[1.0738,0.12851,0...|            1.2156|  2.4393147799442896|\n",
      "|    102|   2|[1.0663,0.0688,0....|         0.0039412|  0.4636442058004643|\n",
      "|    102|   8|[1.6494,0.11315,0...|           0.51542|  0.8798071445645164|\n",
      "|    102|  10|[0.9068,0.06267,0...|           0.22242|  0.8798071445645164|\n",
      "|    102|  12|[1.6099,0.1366,0....|            1.6988|  0.8798071445645164|\n",
      "|    104|   3|[1.0203,0.030599,...|           0.32867| 0.22446443815899586|\n",
      "|    104|   5|[1.136,0.07224,0....|           0.32333|  0.8798071445645164|\n",
      "+-------+----+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtReg_predict.select('Company','Time','Features','Financial Distress','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.085289192053485"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型评估\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "reg_evaluator = RegressionEvaluator(labelCol='Financial Distress',predictionCol = 'prediction',metricName='rmse')\n",
    "rmse = reg_evaluator.evaluate(dtReg_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44734030206570463"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(labelCol='Financial Distress',predictionCol = 'prediction',metricName='r2').evaluate(dtReg_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.689803464316605"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TrainValidation\n",
    "paramGrid = ParamGridBuilder().addGrid(dtReg.maxDepth,[5,10,15,20]) \\\n",
    "                              .addGrid(dtReg.maxBins,[10,15,20,25]).build()\n",
    "\n",
    "dtRegTVS = TrainValidationSplit(estimator = dtReg,evaluator=reg_evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "dtRegTVS_pipeline  = Pipeline(stages=[vecAssembler,dtRegTVS])\n",
    "dtRegTVS_models = dtRegTVS_pipeline.fit(train_df)\n",
    "dtRegTVS_predict = dtRegTVS_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(dtRegTVS_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtRegTVS_models.stages[1].bestModel.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidation\n",
    "dtRegCV = CrossValidator(estimator=dtReg,evaluator=reg_evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "dtRegCV_pipeline = Pipeline(stages=[vecAssembler,dtRegCV])\n",
    "dtRegCV_models = dtRegCV_pipeline.fit(train_df)\n",
    "dtRegCV_predict = dtRegCV_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(dtRegCV_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0442384266167395"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 GBT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0434733167219485"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbtReg = GBTRegressor(labelCol='Financial Distress',featuresCol='Features')\n",
    "gbtReg_pipeline = Pipeline(stages=[vecAssembler,gbtReg])\n",
    "gbtReg_models = gbtReg_pipeline.fit(train_df)\n",
    "gbtReg_predict = gbtReg_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtReg_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/share/Anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-d34d01a55f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                estimatorParamMaps = paramGrid,trainRatio=0.7)\n\u001b[1;32m      6\u001b[0m \u001b[0mgbtRegTVS_pipeline\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvecAssembler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgbtRegTVS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgbtRegTVS_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbtRegTVS_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgbtRegTVS_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbtRegTVS_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbtRegTVS_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark-2.3.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/share/spark-2.3.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark-2.3.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/share/spark-2.3.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/Anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/Anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder().addGrid(gbtReg.maxDepth,[5,10,15,20]) \\\n",
    "                              .addGrid(gbtReg.maxBins,[10,15,20,25])  \\\n",
    "                              .addGrid(gbtReg.maxIter,[10,30]).build()\n",
    "\n",
    "gbtRegTVS = TrainValidationSplit(estimator = gbtReg,evaluator=reg_evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "gbtRegTVS_pipeline  = Pipeline(stages=[vecAssembler,gbtRegTVS])\n",
    "gbtRegTVS_models = gbtRegTVS_pipeline.fit(train_df)\n",
    "gbtRegTVS_predict = gbtRegTVS_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtRegTVS_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidation\n",
    "gbtRegCV = CrossValidator(estimator=gbtReg,evaluator=reg_evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "gbtRegCV_pipeline = Pipeline(stages=[vecAssembler,gbtRegCV])\n",
    "gbtRegCV_models = gbtRegCV_pipeline.fit(train_df)\n",
    "gbtRegCV_predict = gbtRegCV_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtRegCV_predict)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

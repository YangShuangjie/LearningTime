{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.准备环境与路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CreateSparkContext():\n",
    "#     '''\n",
    "#     spark配置：\n",
    "#     1.显示在spark或 hadoop-yarn UI界面的App名称\n",
    "#     2.设置不显示spark执行进度以免界面太乱\n",
    "#     '''\n",
    "#     sparkConf = SparkConf().setAppName('Finacial Distress Prediction')  \\\n",
    "#             .set('spark.ui.showConsoleProgress','false') \n",
    "#     sc = SparkContext(conf=sparkConf)\n",
    "#     print('master='+sc.master)\n",
    "#     SetLogger(sc) #设置不要显示太多信息\n",
    "#     SetPath(sc) # 设置文件读取路径\n",
    "#     return sc\n",
    "\n",
    "# def SetLogger(sc):\n",
    "#     logger = sc._jvm.org.apache.log4j\n",
    "#     logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "#     logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "#     logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)\n",
    "    \n",
    "# def SetPath(sc):\n",
    "#     if sc.master[0:5]==\"local\":\n",
    "#         Path = \"file:/home/hadoop/eclipse-workspace/FinacialDistress/\"\n",
    "#     else:\n",
    "#         Path = \"hdfs://ubuntu:9000/sparkproject/FinacialDistress/\"\n",
    "#     return Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "# Path = SetPath(sc)\n",
    "Path = \"file:/home/hadoop/eclipse-workspace/FinancialDistressPre/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.读取数据并整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Financial Distress: string (nullable = true)\n",
      " |-- x1: string (nullable = true)\n",
      " |-- x2: string (nullable = true)\n",
      " |-- x3: string (nullable = true)\n",
      " |-- x4: string (nullable = true)\n",
      " |-- x5: string (nullable = true)\n",
      " |-- x6: string (nullable = true)\n",
      " |-- x7: string (nullable = true)\n",
      " |-- x8: string (nullable = true)\n",
      " |-- x9: string (nullable = true)\n",
      " |-- x10: string (nullable = true)\n",
      " |-- x11: string (nullable = true)\n",
      " |-- x12: string (nullable = true)\n",
      " |-- x13: string (nullable = true)\n",
      " |-- x14: string (nullable = true)\n",
      " |-- x15: string (nullable = true)\n",
      " |-- x16: string (nullable = true)\n",
      " |-- x17: string (nullable = true)\n",
      " |-- x18: string (nullable = true)\n",
      " |-- x19: string (nullable = true)\n",
      " |-- x20: string (nullable = true)\n",
      " |-- x21: string (nullable = true)\n",
      " |-- x22: string (nullable = true)\n",
      " |-- x23: string (nullable = true)\n",
      " |-- x24: string (nullable = true)\n",
      " |-- x25: string (nullable = true)\n",
      " |-- x26: string (nullable = true)\n",
      " |-- x27: string (nullable = true)\n",
      " |-- x28: string (nullable = true)\n",
      " |-- x29: string (nullable = true)\n",
      " |-- x30: string (nullable = true)\n",
      " |-- x31: string (nullable = true)\n",
      " |-- x32: string (nullable = true)\n",
      " |-- x33: string (nullable = true)\n",
      " |-- x34: string (nullable = true)\n",
      " |-- x35: string (nullable = true)\n",
      " |-- x36: string (nullable = true)\n",
      " |-- x37: string (nullable = true)\n",
      " |-- x38: string (nullable = true)\n",
      " |-- x39: string (nullable = true)\n",
      " |-- x40: string (nullable = true)\n",
      " |-- x41: string (nullable = true)\n",
      " |-- x42: string (nullable = true)\n",
      " |-- x43: string (nullable = true)\n",
      " |-- x44: string (nullable = true)\n",
      " |-- x45: string (nullable = true)\n",
      " |-- x46: string (nullable = true)\n",
      " |-- x47: string (nullable = true)\n",
      " |-- x48: string (nullable = true)\n",
      " |-- x49: string (nullable = true)\n",
      " |-- x50: string (nullable = true)\n",
      " |-- x51: string (nullable = true)\n",
      " |-- x52: string (nullable = true)\n",
      " |-- x53: string (nullable = true)\n",
      " |-- x54: string (nullable = true)\n",
      " |-- x55: string (nullable = true)\n",
      " |-- x56: string (nullable = true)\n",
      " |-- x57: string (nullable = true)\n",
      " |-- x58: string (nullable = true)\n",
      " |-- x59: string (nullable = true)\n",
      " |-- x60: string (nullable = true)\n",
      " |-- x61: string (nullable = true)\n",
      " |-- x62: string (nullable = true)\n",
      " |-- x63: string (nullable = true)\n",
      " |-- x64: string (nullable = true)\n",
      " |-- x65: string (nullable = true)\n",
      " |-- x66: string (nullable = true)\n",
      " |-- x67: string (nullable = true)\n",
      " |-- x68: string (nullable = true)\n",
      " |-- x69: string (nullable = true)\n",
      " |-- x70: string (nullable = true)\n",
      " |-- x71: string (nullable = true)\n",
      " |-- x72: string (nullable = true)\n",
      " |-- x73: string (nullable = true)\n",
      " |-- x74: string (nullable = true)\n",
      " |-- x75: string (nullable = true)\n",
      " |-- x76: string (nullable = true)\n",
      " |-- x77: string (nullable = true)\n",
      " |-- x78: string (nullable = true)\n",
      " |-- x79: string (nullable = true)\n",
      " |-- x80: string (nullable = true)\n",
      " |-- x81: string (nullable = true)\n",
      " |-- x82: string (nullable = true)\n",
      " |-- x83: string (nullable = true)\n",
      "\n",
      "+-------+----+------------------+-------+---------+\n",
      "|Company|Time|Financial Distress|     x1|       x2|\n",
      "+-------+----+------------------+-------+---------+\n",
      "|      1|   1|          0.010636|  1.281| 0.022934|\n",
      "|      1|   2|          -0.45597|   1.27|0.0064542|\n",
      "|      1|   3|          -0.32539| 1.0529|-0.059379|\n",
      "|      1|   4|          -0.56657| 1.1131|-0.015229|\n",
      "|      2|   1|            1.3573| 1.0623|  0.10702|\n",
      "|      2|   2|         0.0071875| 1.0558| 0.081916|\n",
      "|      2|   3|            1.2002|0.97059| 0.076064|\n",
      "|      2|   4|            2.2348|  1.059|   0.1302|\n",
      "|      2|   5|            1.3405| 1.1245|  0.14784|\n",
      "|      2|   6|            2.0474| 1.5998|  0.26246|\n",
      "|      2|   7|            2.3459| 1.5756|   0.2621|\n",
      "|      2|   8|            2.2499| 1.5443|  0.24091|\n",
      "|      2|   9|            2.2826| 1.7217|  0.21525|\n",
      "|      2|  10|            1.7982| 1.7232|  0.20863|\n",
      "|      2|  11|            2.8277| 1.6307|  0.18623|\n",
      "|      2|  12|            2.9413| 2.1723|  0.31945|\n",
      "|      2|  13|            2.6598| 2.0924|  0.30348|\n",
      "|      2|  14|             2.232| 2.2263|  0.32588|\n",
      "|      3|   1|           -1.6599| 0.8744|-0.034676|\n",
      "|      4|   1|           0.02275|0.83537|  0.06431|\n",
      "+-------+----+------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .load(Path+\"data/FinacialDistress.csv\")\n",
    "\n",
    "\n",
    "\n",
    "raw_df.count()\n",
    "\n",
    "type(raw_df)\n",
    "\n",
    "raw_df.printSchema()\n",
    "\n",
    "raw_df.select('Company','Time','Financial Distress','x1','x2').show()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "import pyspark.sql.types\n",
    "def replace_question(col): # df.na.fill() 或者df.fillna()\n",
    "    return ('0' if col==\"?\" else col)\n",
    "replace_question = udf(replace_question)\n",
    "\n",
    "rawDF = raw_df.select(['Company']+[replace_question(col(\"Time\")).cast('int').alias('Time')] +\n",
    "                       [replace_question(col(c)).cast('double').alias(c) for c in raw_df.columns[2:]])\n",
    "\n",
    "\n",
    "df = rawDF.withColumn('Financial Distress01',(rawDF['Financial Distress']>-0.5).cast('int'))\n",
    "\n",
    "train_df,test_df = df.randomSplit([0.7,0.3])\n",
    "# train_df.cache()\n",
    "# test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6996187363834423"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.处理字符串分类特征\n",
    "* 利用StringIndexer将文字分类特征转换为数值分类特征\n",
    "* 利用OneHotEncoder将数值分类特征转换为Vector如(0,0,0,1,0,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "# # -----StringIndexer----\n",
    "# category = StringIndexer(inputCol='inputStr',outputCOl='outputNum') #StringIndexer是一个Estimator\n",
    "# categoryTransformer = category.fit(df) #调用fit方法后生成一个Transformer\n",
    "# categoryTransformer.labels #即为分类字符串\n",
    "# #将Transformer应用到train_df和test_df\n",
    "# trainDF = categoryTransformer.transform(train_df)\n",
    "# testDF = categoryTransformer.transform(test_df)\n",
    "\n",
    "# # -------OneHotEncoder------\n",
    "# encoder = OneHotEncoder(inputCol='outputNum',outputCol='outputVec',dropLast=False)\n",
    "# train_DF = encoder.transform(trainDF)\n",
    "# testDF = encoder.transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.使用 VectorAssembler 将所有特征集成为一个特征Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assemblers = df.columns[3:-1]\n",
    "vecAssembler = VectorAssembler(inputCols = assemblers,outputCol= 'Features')\n",
    "trainDF = vecAssembler.transform(train_df)\n",
    "# testDF = vecAssembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Features|\n",
      "+--------------------+\n",
      "|[1.281,0.022934,0...|\n",
      "|[1.1131,-0.015229...|\n",
      "|[1.4809,0.11739,0...|\n",
      "|[1.3854,0.1013,0....|\n",
      "|[1.267,0.053775,0...|\n",
      "|[1.2675,0.059595,...|\n",
      "|[1.3881,0.10079,0...|\n",
      "|[1.4323,0.083595,...|\n",
      "|[1.3399,0.066151,...|\n",
      "|[1.3167,0.061076,...|\n",
      "|[1.3457,0.049124,...|\n",
      "|[1.3107,0.028949,...|\n",
      "|[1.3833,0.087312,...|\n",
      "|[1.3833,0.013043,...|\n",
      "|[1.0246,0.13107,0...|\n",
      "|[1.1007,0.059918,...|\n",
      "|[1.3134,0.065683,...|\n",
      "|[0.98085,0.014711...|\n",
      "|[0.88577,0.051745...|\n",
      "|[0.94864,0.040847...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Features=DenseVector([1.281, 0.0229, 0.8745, 1.2164, 0.0609, 0.1883, 0.5251, 0.0189, 0.1828, 0.0064, 0.8582, 2.0058, 0.1255, 6.9706, 4.6512, 0.0501, 2.1984, 0.0183, 0.025, 0.0273, 1.4173, 9.5554, 0.1487, 0.67, 214.76, 12.641, 6.4607, 0.0438, 0.2046, 0.3518, 8.3161, 0.2892, 0.7661, 2.5825, 77.4, 0.0267, 1.6307, 0.015, 0.0055, 0.1273, 9.6951, -0.7362, 0.9856, 0.1802, 1.5006, 0.0262, 7.0513, 1174.9, 5.3399, 0.8513, 12.837, 0.0617, 0.1809, 209.87, -0.5826, 0.471, 0.1099, 0.0, 0.0, 0.2201, 7.1241, 15.381, 3.2702, 17.872, 34.692, 30.087, 12.8, 7991.4, 364.95, 15.8, 61.476, 4.0, 36.0, 85.437, 27.07, 26.102, 16.0, 16.0, 0.2, 22.0, 0.0604, 30.0, 49.0]))]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select('Features').show()\n",
    "trainDF.select('Features').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.使用DecisionTreeClassifier二元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtClassifier = DecisionTreeClassifier(labelCol='Financial Distress01',featuresCol='Features',impurity='gini',maxDepth=10,maxBins=14)\n",
    "dtModel = dtClassifier.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPredict = dtModel.transform(testDF) #预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, Time: int, Financial Distress: double, x1: double, x2: double, x3: double, x4: double, x5: double, x6: double, x7: double, x8: double, x9: double, x10: double, x11: double, x12: double, x13: double, x14: double, x15: double, x16: double, x17: double, x18: double, x19: double, x20: double, x21: double, x22: double, x23: double, x24: double, x25: double, x26: double, x27: double, x28: double, x29: double, x30: double, x31: double, x32: double, x33: double, x34: double, x35: double, x36: double, x37: double, x38: double, x39: double, x40: double, x41: double, x42: double, x43: double, x44: double, x45: double, x46: double, x47: double, x48: double, x49: double, x50: double, x51: double, x52: double, x53: double, x54: double, x55: double, x56: double, x57: double, x58: double, x59: double, x60: double, x61: double, x62: double, x63: double, x64: double, x65: double, x66: double, x67: double, x68: double, x69: double, x70: double, x71: double, x72: double, x73: double, x74: double, x75: double, x76: double, x77: double, x78: double, x79: double, x80: double, x81: double, x82: double, x83: double, Financial Distress01: int, Features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtPredict.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.建立机器学习流程pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler,dtClassifier])\n",
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 10 with 145 nodes\n",
      "  If (feature 45 <= 0.0022849999999999997)\n",
      "   If (feature 2 <= 0.734785)\n",
      "    If (feature 22 <= 0.17000500000000002)\n",
      "     If (feature 61 <= 12.175)\n",
      "      If (feature 19 <= 0.25765499999999997)\n",
      "       If (feature 60 <= 6.92695)\n",
      "        If (feature 68 <= -46.2455)\n",
      "         If (feature 16 <= 1.3288)\n",
      "          Predict: 0.0\n",
      "         Else (feature 16 > 1.3288)\n",
      "          Predict: 1.0\n",
      "        Else\n"
     ]
    }
   ],
   "source": [
    "print(pipelineModel.stages[1].toDebugString[:500]) #查看决策树规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePredict = pipelineModel.transform(test_df) #预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|Company|Time|            Features|Financial Distress01|rawPrediction|         probability|prediction|\n",
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|      1|   2|[1.27,0.0064542,0...|                   1|    [0.0,4.0]|           [0.0,1.0]|       1.0|\n",
      "|      1|   3|[1.0529,-0.059379...|                   1|   [15.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|     10|  12|[1.5795,0.17996,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|     10|  13|[1.5012,0.078785,...|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|   2|[0.79696,0.090335...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|   6|[0.9303,0.084578,...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|  11|[1.1898,0.10814,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    100|  12|[1.5872,0.0024181...|                   1|    [3.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|    100|  13|[1.3346,-0.1506,0...|                   1|    [0.0,6.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   2|[1.1733,0.11988,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   3|[1.1295,0.053808,...|                   1|  [0.0,208.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   4|[2.2288,0.27663,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   6|[1.3375,0.16029,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    101|   9|[1.0738,0.12851,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|   2|[1.0663,0.0688,0....|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|   8|[1.6494,0.11315,0...|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|  10|[0.9068,0.06267,0...|                   1|  [0.0,323.0]|           [0.0,1.0]|       1.0|\n",
      "|    102|  12|[1.6099,0.1366,0....|                   1| [0.0,1436.0]|           [0.0,1.0]|       1.0|\n",
      "|    104|   3|[1.0203,0.030599,...|                   1|    [0.0,9.0]|           [0.0,1.0]|       1.0|\n",
      "|    104|   5|[1.136,0.07224,0....|                   1|  [3.0,181.0]|[0.01630434782608...|       1.0|\n",
      "+-------+----+--------------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelinePredict.select('Company','Time','Features','Financial Distress01','rawPrediction','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.评估模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',\n",
    "                                          labelCol='Financial Distress01',\n",
    "                                          metricName='areaUnderROC')\n",
    "auc = evaluator.evaluate(pipelinePredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042644896303433"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.寻找最佳模型\n",
    "\n",
    "## 8.1使用TrainValidationSplit寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder().addGrid(dtClassifier.impurity,[\"gini\",\"entropy\"]) \\\n",
    "                .addGrid(dtClassifier.maxDepth,[5,10,15,20]) \\\n",
    "                .addGrid(dtClassifier.maxBins,[10,15,20,25]).build()\n",
    "dtTVS = TrainValidationSplit(estimator = dtClassifier,evaluator=evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "dtTVS_pipeline = Pipeline(stages=[vecAssembler,dtTVS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtTVS_models = dtTVS_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 53 nodes"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = dtTVS_models.stages[1].bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 53 nodes\n",
      "  If (feature 8 <= -0.114315)\n",
      "   If (feature 57 <= -9.549999999999999E-5)\n",
      "    If (feature 10 <= 0.50169)\n",
      "     If (feature 64 <= 34.1225)\n",
      "      Predict: 1.0\n",
      "     Else (feature 64 > 34.1225)\n",
      "      If (feature 0 <= 0.7623949999999999)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 0.7623949999999999)\n",
      "       Predict: 1.0\n",
      "    Else (feature 10 > 0.50169)\n",
      "     If (feature 81 <= 12.5)\n",
      "      Predict: 0\n"
     ]
    }
   ],
   "source": [
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9033390801683483"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtTVS_predict = dtTVS_models.transform(test_df)\n",
    "auc = evaluator.evaluate(dtTVS_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2使用crossValidation寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "dtCV = CrossValidator(estimator=dtClassifier,evaluator=evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "dtCV_pipeline = Pipeline(stages=[vecAssembler,dtCV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtCV_models = dtCV_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c8989b34e2c2d43789c) of depth 5 with 55 nodes"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel_ = dtCV_models.stages[1].bestModel\n",
    "bestModel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9061279853962781"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtCV_predict = dtCV_models.transform(test_df)\n",
    "auc = evaluator.evaluate(dtCV_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241099881635253"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier(labelCol = 'Financial Distress01',featuresCol='Features',numTrees = 10)\n",
    "rf_pipeline = Pipeline(stages=[vecAssembler,rfClassifier])\n",
    "rf_models = rf_pipeline.fit(train_df)\n",
    "rfPredict = rf_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfPredict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 使用TrainValidationSplit寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9373918783574616"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(rfClassifier.impurity,[\"gini\",\"entropy\"]) \\\n",
    "                .addGrid(rfClassifier.maxDepth,[5,10,15]) \\\n",
    "                .addGrid(rfClassifier.maxBins,[10,15,20]) \\\n",
    "                .addGrid(rfClassifier.numTrees,[10,20]).build()\n",
    "rfTVS = TrainValidationSplit(estimator = rfClassifier,evaluator=evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "rfTVS_pipeline  = Pipeline(stages=[vecAssembler,rfTVS])\n",
    "rfTVS_models = rfTVS_pipeline.fit(train_df)\n",
    "rfTVS_predict = rfTVS_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfTVS_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(rfTVS_models.stages[1].bestModel.totalNumNodes)\n",
    "print(rfTVS_models.stages[1].bestModel.treeWeights)\n",
    "print(rfTVS_models.stages[1].bestModel.getNumTrees)\n",
    "#  'numClasses',\n",
    "#  'numFeatures',\n",
    "#  'numTrees',\n",
    "#  'params',\n",
    "#  'thresholds',\n",
    "#  'toDebugString',\n",
    "#  'totalNumNodes',\n",
    "#  'treeWeights',\n",
    "#  'trees',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2使用crossValidation寻找最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfCV = CrossValidator(estimator=rfClassifier,evaluator=evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "rfCV_pipeline = Pipeline(stages=[vecAssembler,rfCV])\n",
    "rfCV_models = rfCV_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9351706302925814"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfCV_predict = rfCV_models.transform(test_df)\n",
    "auc = evaluator.evaluate(rfCV_predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 决策树回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dtReg = DecisionTreeRegressor(labelCol='Financial Distress',featuresCol='Features')\n",
    "dtReg_pipeline = Pipeline(stages=[vecAssembler,dtReg])\n",
    "dtReg_model = dtReg_pipeline.fit(train_df)\n",
    "dtReg_predict = dtReg_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+------------------+--------------------+\n",
      "|Company|Time|            Features|Financial Distress|          prediction|\n",
      "+-------+----+--------------------+------------------+--------------------+\n",
      "|      1|   2|[1.27,0.0064542,0...|          -0.45597|-0.42818420844155847|\n",
      "|      1|   3|[1.0529,-0.059379...|          -0.32539|-0.42818420844155847|\n",
      "|     10|  12|[1.5795,0.17996,0...|            1.1553|  0.8798071445645164|\n",
      "|     10|  13|[1.5012,0.078785,...|           0.31012|  0.8798071445645164|\n",
      "|    100|   2|[0.79696,0.090335...|           0.13046| 0.22446443815899586|\n",
      "|    100|   6|[0.9303,0.084578,...|           0.47346| 0.22446443815899586|\n",
      "|    100|  11|[1.1898,0.10814,0...|           0.04456|  0.4636442058004643|\n",
      "|    100|  12|[1.5872,0.0024181...|           -0.1284|0.012763109456521747|\n",
      "|    100|  13|[1.3346,-0.1506,0...|          -0.48243|-0.42818420844155847|\n",
      "|    101|   2|[1.1733,0.11988,0...|            1.2129|   1.411190434322034|\n",
      "|    101|   3|[1.1295,0.053808,...|            1.0134|  0.8798071445645164|\n",
      "|    101|   4|[2.2288,0.27663,0...|            1.3322|  0.8798071445645164|\n",
      "|    101|   6|[1.3375,0.16029,0...|            1.1882|   1.411190434322034|\n",
      "|    101|   9|[1.0738,0.12851,0...|            1.2156|  2.4393147799442896|\n",
      "|    102|   2|[1.0663,0.0688,0....|         0.0039412|  0.4636442058004643|\n",
      "|    102|   8|[1.6494,0.11315,0...|           0.51542|  0.8798071445645164|\n",
      "|    102|  10|[0.9068,0.06267,0...|           0.22242|  0.8798071445645164|\n",
      "|    102|  12|[1.6099,0.1366,0....|            1.6988|  0.8798071445645164|\n",
      "|    104|   3|[1.0203,0.030599,...|           0.32867| 0.22446443815899586|\n",
      "|    104|   5|[1.136,0.07224,0....|           0.32333|  0.8798071445645164|\n",
      "+-------+----+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtReg_predict.select('Company','Time','Features','Financial Distress','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "reg_evaluator = RegressionEvaluator(labelCol='Financial Distress',predictionCol = 'prediction',metricName='rmse')\n",
    "rmse = reg_evaluator.evaluate(dtReg_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44734030206570463"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(labelCol='Financial Distress',predictionCol = 'prediction',metricName='r2').evaluate(dtReg_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4569681896173268"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TrainValidation\n",
    "paramGrid = ParamGridBuilder().addGrid(dtReg.maxDepth,[5,10,15]) \\\n",
    "                              .addGrid(dtReg.maxBins,[10,15,20]).build()\n",
    "\n",
    "dtRegTVS = TrainValidationSplit(estimator = dtReg,evaluator=reg_evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "dtRegTVS_pipeline  = Pipeline(stages=[vecAssembler,dtRegTVS])\n",
    "dtRegTVS_models = dtRegTVS_pipeline.fit(train_df)\n",
    "dtRegTVS_predict = dtRegTVS_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(dtRegTVS_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtRegTVS_models.stages[1].bestModel.depth\n",
    "dtRegTVS_models.stages[1].bestModel.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidation\n",
    "dtRegCV = CrossValidator(estimator=dtReg,evaluator=reg_evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "dtRegCV_pipeline = Pipeline(stages=[vecAssembler,dtRegCV])\n",
    "dtRegCV_models = dtRegCV_pipeline.fit(train_df)\n",
    "dtRegCV_predict = dtRegCV_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(dtRegCV_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0442384266167395"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 GBT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbtReg = GBTRegressor(labelCol='Financial Distress',featuresCol='Features')\n",
    "gbtReg_pipeline = Pipeline(stages=[vecAssembler,gbtReg])\n",
    "gbtReg_models = gbtReg_pipeline.fit(train_df)\n",
    "gbtReg_predict = gbtReg_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtReg_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2146728441588632"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder().addGrid(gbtReg.maxDepth,[5,10]) \\\n",
    "                              .addGrid(gbtReg.maxBins,[10,15])  \\\n",
    "                              .addGrid(gbtReg.maxIter,[10,30]).build()\n",
    "\n",
    "gbtRegTVS = TrainValidationSplit(estimator = gbtReg,evaluator=reg_evaluator,\n",
    "                               estimatorParamMaps = paramGrid,trainRatio=0.7)\n",
    "gbtRegTVS_pipeline  = Pipeline(stages=[vecAssembler,gbtRegTVS])\n",
    "gbtRegTVS_models = gbtRegTVS_pipeline.fit(train_df)\n",
    "gbtRegTVS_predict = gbtRegTVS_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtRegTVS_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtRegTVS_models.stages[1].bestModel.totalNumNodes\n",
    "gbtRegTVS_models.stages[1].bestModel.getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidation\n",
    "gbtRegCV = CrossValidator(estimator=gbtReg,evaluator=reg_evaluator,\n",
    "                    estimatorParamMaps = paramGrid,numFolds=3)\n",
    "\n",
    "gbtRegCV_pipeline = Pipeline(stages=[vecAssembler,gbtRegCV])\n",
    "gbtRegCV_models = gbtRegCV_pipeline.fit(train_df)\n",
    "gbtRegCV_predict = gbtRegCV_models.transform(test_df)\n",
    "rmse = reg_evaluator.evaluate(gbtRegCV_predict)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoBatchedSerializer',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PickleSerializer',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_binary_mathfunctions',\n",
       " '_collect_list_doc',\n",
       " '_collect_set_doc',\n",
       " '_create_binary_mathfunction',\n",
       " '_create_function',\n",
       " '_create_udf',\n",
       " '_create_window_function',\n",
       " '_functions',\n",
       " '_functions_1_4',\n",
       " '_functions_1_6',\n",
       " '_functions_2_1',\n",
       " '_functions_deprecated',\n",
       " '_lit_doc',\n",
       " '_message',\n",
       " '_string_functions',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_window_functions',\n",
       " '_wrap_deprecated_function',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'add_months',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'asc',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'blacklist',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'encode',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'first',\n",
       " 'floor',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hypot',\n",
       " 'ignore_unicode_prefix',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'math',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'ntile',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'second',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'year']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.sql.functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
